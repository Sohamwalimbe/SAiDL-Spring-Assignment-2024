# -*- coding: utf-8 -*-
"""CV_final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xl4KAYP9SLyhcto2F6MZJQqQ54WRWmJH
"""

# Commented out IPython magic to ensure Python compatibility.
import torch
import torch.nn as nn
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
import torch.optim
from torch.distributions.beta import Beta

# %matplotlib inline

# Variational Autoencoder implementation
class VarAE(nn.Module):
  def __init__(self,input_size,hidden_size,latent_size):
    super().__init__()

    self.i = input_size
    self.h = hidden_size
    self.l = latent_size

    self.encoderl1 = nn.Sequential(
        nn.Linear(self.i,self.h),
        nn.ReLU()
    )

    self.encodermu = nn.Linear(self.h,self.l)
    self.encoderlogvar = nn.Linear(self.h,self.l)

    self.decoder = nn.Sequential(
        nn.Linear(self.l,self.h),
        nn.ReLU(),
        nn.Linear(self.h,self.i),
        nn.Sigmoid()
    )

  def forward(self,x):

    a = self.encoderl1(x)
    mu = self.encodermu(a)
    logvar = self.encoderlogvar(a)
    eps = torch.randn_like(mu)
    std = torch.exp(0.5 * logvar)
    latent = mu + eps * std

    # Modification of the forward function
    # mu_sampled = torch.normal(mu, torch.ones_like(mu))
    # logvar_sampled = torch.normal(logvar, torch.ones_like(logvar))
    # std = torch.exp(0.5 * logvar_sampled)
    # eps = torch.randn_like(std)
    # latent = mu_sampled + eps * std

    res = self.decoder(latent)

    return res,mu,logvar

# Total loss calculation
def total_loss(x,x_pred,mean,logvar):
  #reconstruction_loss = nn.BCELoss(reduction="sum")(x_pred,x)
  reconstruction_loss = nn.MSELoss(reduction="sum")(x_pred,x)
  kl_divergence = 0.5*torch.sum(logvar.exp() + mean**2 -logvar-1)
  return reconstruction_loss + kl_divergence

# Preparing the data
transform = transforms.Compose([transforms.ToTensor()])

train_data = datasets.MNIST(root='./data', train=True, transform=transform, download=True)
train_dl = DataLoader(train_data, batch_size=512, shuffle=True)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Trainer class defined for simplicity
class Trainer:
  def __init__(self,model,train_dl):
    self.model = model.to(device)
    self.train_dl = train_dl
    self.optimizer = torch.optim.Adam(self.model.parameters())

  def fit(self,epochs):
    self.model.train()
    for i in range(epochs):
      train_loss = self.train_per_epoch()
      print(f"Epoch:{i+1}, Train loss:{train_loss}")

  def train_per_epoch(self):
    sum_loss = 0
    for i,(img,_) in enumerate(self.train_dl):
      img = img.view(img.size(0), -1).to(device)
      reconstructed,mu,sigma = self.model(img)
      self.optimizer.zero_grad()
      loss = total_loss(img,reconstructed,mu,sigma)
      loss.backward()
      self.optimizer.step()
      sum_loss += loss.item()
    return sum_loss/len(img)

  def evaluate(self,name):
    self.model.eval()

    # Techniques for sampling

    sample = torch.randn(1, self.model.l).to(device)

    # sample = torch.normal(mean=torch.ones(1, self.model.l), std=2*torch.ones(1, self.model.l)).to(device)

    # alpha = torch.tensor(0.125).to(device)
    # beta = torch.tensor(0.125).to(device)
    # beta_distribution = Beta(alpha, beta)
    # sample = beta_distribution.sample((1, self.model.l))

    with torch.no_grad():
      output = self.model.decoder(sample).view(1, 28, 28)
    plt.imshow(output.cpu().view(28, 28), cmap='gray')
    plt.axis('off')
    plt.savefig(name, bbox_inches='tight', pad_inches=0)
    plt.show()

model = VarAE(784,392,100)
trainer = Trainer(model,train_dl)

trainer.fit(epochs=100)

trainer.evaluate("normal_5")