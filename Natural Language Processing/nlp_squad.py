# -*- coding: utf-8 -*-
"""NLP_squad.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1P9QObvGoY29_SuO53l_ZA8LJbZ5pSxLg
"""

!pip install datasets

# Loading the dataset

from datasets import load_dataset
dataset = load_dataset('squad')

# Used T5 Tokenizer
from transformers import T5Tokenizer
t5_tokenizer = T5Tokenizer.from_pretrained('t5-base', model_max_length=512)

from datasets import Dataset

# Preprocessing
def tokenize_example(example):
    context = example['context']
    question = example['question']
    answer = example['answers']['text'][0]
    input_text = f"question: {question} context: {context}"
    target_text = answer

    input_encodings = t5_tokenizer.encode_plus(
        input_text,
        add_special_tokens=True,
        max_length=512,
        padding="max_length",
        truncation=True,
        return_attention_mask=True,
        return_tensors="pt"
    )

    target_encodings = t5_tokenizer.encode_plus(
        target_text,
        add_special_tokens=True,
        max_length=512,
        padding="max_length",
        truncation=True,
        return_attention_mask=True,
        return_tensors="pt"
    )

    return {
        'input_ids': input_encodings['input_ids'].squeeze(),
        'attention_mask': input_encodings['attention_mask'].squeeze(),
        'decoder_input_ids': target_encodings['input_ids'].squeeze(),
        'decoder_attention_mask': target_encodings['attention_mask'].squeeze()
    }

tokenized_datasets = {split: [] for split in dataset.keys()}

for split in dataset.keys():
    for example in dataset[split]:
        tokenized_example = tokenize_example(example)
        tokenized_datasets[split].append(tokenized_example)

tokenized_dataset = {split: Dataset.from_dict({k: [d[k] for d in tokenized_datasets[split]] for k in tokenized_datasets[split][0]}) for split in tokenized_datasets}

# Import the model from a seperate file
from moe_model import combinedNetwork

from torch.utils.data.dataloader import default_collate
import torch

# To convert data to Tensor and batch it
def collate_fn(batch):
    batched_data = {}

    for key in batch[0].keys():
        if isinstance(batch[0][key], torch.Tensor):
            batched_data[key] = default_collate([item[key] for item in batch])
        else:
            batched_data[key] = torch.tensor([item[key] for item in batch], dtype=torch.long)

    return batched_data

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

print(t5_tokenizer.vocab_size)

from torch.utils.data import DataLoader

train_loader = DataLoader(tokenized_dataset["train"], batch_size=8, shuffle=True,collate_fn=collate_fn)
model = combinedNetwork(1,25,10,1,4).to(device)

optimizer = torch.optim.Adam(model.parameters(), lr=5e-5)
# Ran out of time, MSELoss is not a good choice here
# Should use CrossEntropyLoss for all the tokens
criterion = torch.nn.MSELoss()

num_epochs = 3

for epoch in range(num_epochs):
    for batch in train_loader:
        input_ids = batch['input_ids'].to(device)
        input_ids = input_ids.unsqueeze(-1)
        decoder_input_ids = batch['decoder_input_ids'].to(device)

        optimizer.zero_grad()

        outputs = model(input_ids)
        outputs = outputs.squeeze(-1)

        loss = criterion(outputs.float(), decoder_input_ids.float())
        loss.backward()
        optimizer.step()

        print(f'Batch Loss: {loss.item():.4f}')

test_loader = DataLoader(tokenized_dataset["validation"], batch_size=8, shuffle=True,collate_fn=collate_fn)
test_loss = 0.0

model.eval()
i=0 # Ran out of time, just to check if the code is working

with torch.no_grad():
    for batch in test_loader:
        i=i+1
        input_ids = batch['input_ids'].to(device)
        input_ids = input_ids.unsqueeze(-1)
        decoder_input_ids = batch['decoder_input_ids'].to(device)

        outputs = model(input_ids)
        outputs = outputs.squeeze(-1)

        loss = criterion(outputs, decoder_input_ids)
        test_loss += loss.item()
        if i>100: break

avg_test_loss = test_loss / len(test_loader)
print(f'Test Loss: {avg_test_loss:.4f}')